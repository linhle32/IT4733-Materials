{"paragraphs":[{"text":"%md\n\n<h2> Working with PySpark DataFrame </h2>\n\nMinimally, a data analytical project has the following steps:\n\n1. **Form the project**. This means to establish the question to solve like what to model or to explain. Then, find or create a dataset that is able to address the analysis.\n2. **Data processing**. In this step, we transform the data to address any issues. The data should be ready for modeling after this process.\n3. **Modeling**. This step includes select the appropriate models for the data and the task, train and finetune the models if needed. Depending on the purpose of the project, one or a few models will be selected as finalists.\n4. **Publish the results**. Any findings or well-trained models from the previous steps should be documented into a report, paper, product, etc.\n\nThis module focus on step 2. In general, processing should come after some preliminary analysis, however, those are out of scope for our class. Instead, we will learn to deal with the most common issues in data\n\n<h3> Loading Data </h3>\n\nWe have learned to load data into our spark session in the previous module. In this module, we will use a student dataset which I created. The data consists of the following columns:\n\n- StudentID\n- FirstName\n- LastName\n- Major\n- HighSchoolGPA\n- AvgDailyStudyTime\n- TotalAbsence (of the first semester)\n- FamilyIncome\n- State\n- isHonor\n\nThe isHonor are the targets in this data. In other words, we want to predict if a student is an honor student based on the other features in data.\n\nWe first load data as a DataFrame. The options we used:\n- header='True': this data comes with columns' names, so we set header as True \n- inferSchema='True': instructs spark to determine the type of each column by itself\n- delimiter=',': the character that seperate fields in a data file. csv files use \",\"\n\nThe loaded data is store in a DataFrame named **students**","user":"anonymous","dateUpdated":"2023-01-24T21:19:36+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2> Working with PySpark DataFrame </h2>\n<p>Minimally, a data analytical project has the following steps:</p>\n<ol>\n<li><strong>Form the project</strong>. This means to establish the question to solve like what to model or to explain. Then, find or create a dataset that is able to address the analysis.</li>\n<li><strong>Data processing</strong>. In this step, we transform the data to address any issues. The data should be ready for modeling after this process.</li>\n<li><strong>Modeling</strong>. This step includes select the appropriate models for the data and the task, train and finetune the models if needed. Depending on the purpose of the project, one or a few models will be selected as finalists.</li>\n<li><strong>Publish the results</strong>. Any findings or well-trained models from the previous steps should be documented into a report, paper, product, etc.</li>\n</ol>\n<p>This module focus on step 2. In general, processing should come after some preliminary analysis, however, those are out of scope for our class. Instead, we will learn to deal with the most common issues in data</p>\n<h3> Loading Data </h3>\n<p>We have learned to load data into our spark session in the previous module. In this module, we will use a student dataset which I created. The data consists of the following columns:</p>\n<ul>\n<li>StudentID</li>\n<li>FirstName</li>\n<li>LastName</li>\n<li>Major</li>\n<li>HighSchoolGPA</li>\n<li>AvgDailyStudyTime</li>\n<li>TotalAbsence (of the first semester)</li>\n<li>FamilyIncome</li>\n<li>State</li>\n<li>isHonor</li>\n</ul>\n<p>The isHonor are the targets in this data. In other words, we want to predict if a student is an honor student based on the other features in data.</p>\n<p>We first load data as a DataFrame. The options we used:</p>\n<ul>\n<li>header='True': this data comes with columns' names, so we set header as True</li>\n<li>inferSchema='True': instructs spark to determine the type of each column by itself</li>\n<li>delimiter=',': the character that seperate fields in a data file. csv files use &ldquo;,&rdquo;</li>\n</ul>\n<p>The loaded data is store in a DataFrame named <strong>students</strong></p>\n"}]},"apps":[],"jobName":"paragraph_1674595176365_-22082895","id":"20230124-200721_1933496500","dateCreated":"2023-01-24T21:19:36+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:7534"},{"text":"%spark2.pyspark\n\nstudents = spark.read.options(header='True',inferSchema='True',delimiter=',').csv(\"/tmp/data/students.csv\")\n","user":"anonymous","dateUpdated":"2023-01-24T21:19:36+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1674595176367_-1514533950","id":"20230124-141029_1046446419","dateCreated":"2023-01-24T21:19:36+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7535"},{"text":"%md\n\nWe can printSchema() and show() to check if the column types are correct\n","user":"anonymous","dateUpdated":"2023-01-24T21:19:36+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>We can printSchema() and show() to check if the column types are correct</p>\n"}]},"apps":[],"jobName":"paragraph_1674595176367_-23369562","id":"20230124-201543_1539268188","dateCreated":"2023-01-24T21:19:36+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7536"},{"text":"%spark2.pyspark\n\nstudents.printSchema()","user":"anonymous","dateUpdated":"2023-01-24T21:19:36+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- StudentID: integer (nullable = true)\n |-- FirstName: string (nullable = true)\n |-- LastName: string (nullable = true)\n |-- Major: string (nullable = true)\n |-- HighSchoolGPA: double (nullable = true)\n |-- FamilyIncome: integer (nullable = true)\n |-- State: string (nullable = true)\n |-- AvgDailyStudyTime: double (nullable = true)\n |-- TotalAbsence: integer (nullable = true)\n |-- isHonor: integer (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1674595176367_-2028682981","id":"20230124-151820_645847926","dateCreated":"2023-01-24T21:19:36+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7537"},{"text":"%spark2.pyspark\nstudents.show()","user":"anonymous","dateUpdated":"2023-01-24T21:19:36+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---------+---------+----------+--------------------+-------------+------------+-----+-----------------+------------+-------+\n|StudentID|FirstName|  LastName|               Major|HighSchoolGPA|FamilyIncome|State|AvgDailyStudyTime|TotalAbsence|isHonor|\n+---------+---------+----------+--------------------+-------------+------------+-----+-----------------+------------+-------+\n|202303595|   Baxter|   Dengler|    Computer Science|         2.82|     45013.0|   TN|             2.01|        14.0|    0.0|\n|202309162|Christian|    Wickey|        Data Science|         3.07|    128358.0|   GA|             5.41|        null|    0.0|\n|202306337|   Lonnie|     Wulff|Software Engineering|         2.68|    112392.0|   GA|             9.57|        13.0|    0.0|\n|202306072| Mitchell|  Deshotel|Software Engineering|         3.21|    190846.0|   GA|             8.57|        16.0|    0.0|\n|202301733|  Linwood|   Willing|Information Techn...|         3.44|    187163.0|   GA|             6.24|        20.0|    0.0|\n|202305362|    Rocco|   Dandrea|Information Techn...|          2.7|     39186.0|   SC|             7.68|        12.0|    1.0|\n|202306465|      Guy|   Mcleroy|Information Techn...|         3.04|    265177.0|   GA|              5.6|        50.0|    0.0|\n|202305188|   Gustav|     Sterk|Software Engineering|         2.66|    184295.0|   FL|             6.49|        18.0|    0.0|\n|202308816|  Seymour|    Ediger|        Data Science|         3.16|     82071.0|   GA|             2.39|        17.0|    0.0|\n|202307074|  Delbert|    Dauria|Software Engineering|         3.18|     30401.0|   AL|             5.58|        14.0|    0.0|\n|202303678|   Donnie|    Riemer|Software Engineering|         2.81|    147583.0|   SC|             7.64|        17.0|    0.0|\n|202302442|     Luke|     Datta|    Computer Science|         2.45|    112614.0|   GA|             5.59|        22.0|    0.0|\n|202302098|     Eric|  Mckellar|Information Techn...|         2.93|    159592.0|   GA|             6.67|        16.0|    0.0|\n|202306309|      Ted|Wisniewski|Information Techn...|         3.06|    117836.0|   TN|            10.68|        24.0|    1.0|\n|202309206|    Smith|    Magnus|        Data Science|         3.08|     81323.0|   FL|             5.98|        18.0|    0.0|\n|202307692|   Jasper| Odriscoll|Information Techn...|         2.25|    204827.0|   SC|             1.91|        21.0|    0.0|\n|202305493|    Riley|    Wynter|Information Techn...|         2.26|    312856.0|   GA|              3.0|        17.0|    0.0|\n|202307947|   Austin|  Galdamez|    Computer Science|         2.07|    123967.0|   GA|             1.27|        29.0|    0.0|\n|202302693|     Bill|      Mize|        Data Science|          2.7|    319900.0|   AL|             7.91|        11.0|    1.0|\n|202304771|  Lindsey|   Basquez|    Computer Science|         2.94|     48870.0|   GA|             6.81|        23.0|    0.0|\n+---------+---------+----------+--------------------+-------------+------------+-----+-----------------+------------+-------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1674595176367_959475586","id":"20230124-152742_1484821132","dateCreated":"2023-01-24T21:19:36+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7538"},{"text":"%md\n\nThe string columns are loaded in correctly. However, we need to change all integer columns to double to avoid errors in modeling later on. \n\nThe code below casts FamilyIncome, TotalAbsence, and isHonor to double types\n","user":"anonymous","dateUpdated":"2023-01-24T21:19:36+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>The string columns are loaded in correctly. However, we need to change all integer columns to double to avoid errors in modeling later on.</p>\n<p>The code below casts FamilyIncome, TotalAbsence, and isHonor to double types</p>\n"}]},"apps":[],"jobName":"paragraph_1674595176367_668762038","id":"20230124-201636_816637435","dateCreated":"2023-01-24T21:19:36+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7539"},{"text":"%spark2.pyspark\n\nfrom pyspark.sql.functions import col\nfrom pyspark.sql.types import DoubleType\n\nstudents = students.withColumn(\"FamilyIncome\",col(\"FamilyIncome\").cast(DoubleType()))\nstudents = students.withColumn(\"TotalAbsence\",col(\"TotalAbsence\").cast(DoubleType()))\nstudents = students.withColumn(\"isHonor\",col(\"isHonor\").cast(DoubleType()))\nstudents.printSchema()","user":"anonymous","dateUpdated":"2023-01-24T21:19:36+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- StudentID: integer (nullable = true)\n |-- FirstName: string (nullable = true)\n |-- LastName: string (nullable = true)\n |-- Major: string (nullable = true)\n |-- HighSchoolGPA: double (nullable = true)\n |-- FamilyIncome: double (nullable = true)\n |-- State: string (nullable = true)\n |-- AvgDailyStudyTime: double (nullable = true)\n |-- TotalAbsence: double (nullable = true)\n |-- isHonor: double (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1674595176367_-321640653","id":"20230124-165845_1320819949","dateCreated":"2023-01-24T21:19:36+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7540"},{"text":"%md\n\nAnother important thing is do before working with data is to determine unnecessary columns and drop them. In this case, StudentID, FirstName, and LastName are likely not useful to predict whether a student is a honor student or not, so we drop them.\n\nAlso note, when making changes like this, it is better to make a copy of the original data so we have it if ever needed.","user":"anonymous","dateUpdated":"2023-01-24T21:19:36+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Another important thing is do before working with data is to determine unnecessary columns and drop them. In this case, StudentID, FirstName, and LastName are likely not useful to predict whether a student is a honor student or not, so we drop them.</p>\n<p>Also note, when making changes like this, it is better to make a copy of the original data so we have it if ever needed.</p>\n"}]},"apps":[],"jobName":"paragraph_1674595176367_-158197016","id":"20230124-201821_1344680715","dateCreated":"2023-01-24T21:19:36+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7541"},{"text":"%spark2.pyspark\n\nstudents_data = students.drop(\"StudentID\", \"FirstName\", \"LastName\")\nstudents_data.printSchema()","user":"anonymous","dateUpdated":"2023-01-24T21:19:36+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- Major: string (nullable = true)\n |-- HighSchoolGPA: double (nullable = true)\n |-- FamilyIncome: double (nullable = true)\n |-- State: string (nullable = true)\n |-- AvgDailyStudyTime: double (nullable = true)\n |-- TotalAbsence: double (nullable = true)\n |-- isHonor: double (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1674595176367_-745524369","id":"20230124-152535_1737355475","dateCreated":"2023-01-24T21:19:36+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7542"},{"text":"%md\n\n<h3>Train Test Splitting</h3>\n\nMachine learning models are mostly developed for predictive purpose. That means models should be able to handle new/future data that they have never seen.\n\nDuring development of models though, we do not have future data. One workaround is to split the whole data we have into two parts, one for model training, and one acts as new/future data for testing and evaluation purpose only.\n\nIn PySpark, we can use DataFrame.randomSplit(). The code below split the data into two portion, 70% for training (students_train) and 30% for testing (students_test). We then do a count() to verify the number of rows in the training set and testing set.","user":"anonymous","dateUpdated":"2023-01-24T21:19:36+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Train Test Splitting</h3>\n<p>Machine learning models are mostly developed for predictive purpose. That means models should be able to handle new/future data that they have never seen.</p>\n<p>During development of models though, we do not have future data. One workaround is to split the whole data we have into two parts, one for model training, and one acts as new/future data for testing and evaluation purpose only.</p>\n<p>In PySpark, we can use DataFrame.randomSplit(). The code below split the data into two portion, 70% for training (students_train) and 30% for testing (students_test). We then do a count() to verify the number of rows in the training set and testing set.</p>\n"}]},"apps":[],"jobName":"paragraph_1674595176367_64347997","id":"20230124-202223_283821740","dateCreated":"2023-01-24T21:19:36+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7543"},{"text":"%spark2.pyspark\n\nstudents_train, students_test = students_data.randomSplit([0.7, 0.3])\n\nstudents_train.count(), students_test.count()","user":"anonymous","dateUpdated":"2023-01-24T21:19:36+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"(677, 323)\n"}]},"apps":[],"jobName":"paragraph_1674595176367_32789769","id":"20230124-152710_324354579","dateCreated":"2023-01-24T21:19:36+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7544"},{"text":"%md\n\n<b>Before continuing, please run the below command in your ssh shell (localhost:4200, log in as maria_dev/maria_dev)</b>\n\n<b>yum install -y numpy</b>\n\n<h4>One Hot Encoding</h4>\n\nVery few models can directly use string attributes. In general, we will try to transform them into meaningful numbers, of which one method is to use **One Hot Encoder.\n\nIn <b>One Hot Encoder</b>, we create a new <b>binary</b> column for <b>each distinct value</b> in the column. This means, a class column that has three values, for example, (low, medium, high), will result in three new binary columns. Here, binary means the columns only have two distinct value, 0 and 1. The value is 1 when the row belong to the corresponding class of the column, and 0 otherwise. For example\n\n| Class |         \n|-------|\n|  low  |\n|medium |\n|  low  |\n|  low  |\n| high  |\n|medium |\n\nis changed into\n\n|isLow|isMedium|isHigh|\n|-----|--------|------|\n|  1  |   0    |  0   |\n|  0  |   1    |  0   |\n|  1  |   0    |  0   |\n|  1  |   0    |  0   |\n|  0  |   0    |  1   |\n|  0  |   1    |  0   |\n\nIn spark, we first need to transform the string columns into indexes using StringIndexer, then we use OneHotEnCoder to perform the enconding process.\n\nBelow is an example of the full process to transform Major to one hot codes. We first import the two needed classes, then use fit() to train and transform() to perform the transformation\n","user":"anonymous","dateUpdated":"2023-01-24T21:19:36+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<b>Before continuing, please run the below command in your ssh shell (localhost:4200, log in as maria_dev/maria_dev)</b>\n<b>yum install -y numpy</b>\n<h4>One Hot Encoding</h4>\n<p>Very few models can directly use string attributes. In general, we will try to transform them into meaningful numbers, of which one method is to use **One Hot Encoder.</p>\n<p>In <b>One Hot Encoder</b>, we create a new <b>binary</b> column for <b>each distinct value</b> in the column. This means, a class column that has three values, for example, (low, medium, high), will result in three new binary columns. Here, binary means the columns only have two distinct value, 0 and 1. The value is 1 when the row belong to the corresponding class of the column, and 0 otherwise. For example</p>\n<table>\n  <thead>\n    <tr>\n      <th>Class </th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>low </td>\n    </tr>\n    <tr>\n      <td>medium </td>\n    </tr>\n    <tr>\n      <td>low </td>\n    </tr>\n    <tr>\n      <td>low </td>\n    </tr>\n    <tr>\n      <td>high </td>\n    </tr>\n    <tr>\n      <td>medium </td>\n    </tr>\n  </tbody>\n</table>\n<p>is changed into</p>\n<table>\n  <thead>\n    <tr>\n      <th>isLow</th>\n      <th>isMedium</th>\n      <th>isHigh</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1 </td>\n      <td>0 </td>\n      <td>0 </td>\n    </tr>\n    <tr>\n      <td>0 </td>\n      <td>1 </td>\n      <td>0 </td>\n    </tr>\n    <tr>\n      <td>1 </td>\n      <td>0 </td>\n      <td>0 </td>\n    </tr>\n    <tr>\n      <td>1 </td>\n      <td>0 </td>\n      <td>0 </td>\n    </tr>\n    <tr>\n      <td>0 </td>\n      <td>0 </td>\n      <td>1 </td>\n    </tr>\n    <tr>\n      <td>0 </td>\n      <td>1 </td>\n      <td>0 </td>\n    </tr>\n  </tbody>\n</table>\n<p>In spark, we first need to transform the string columns into indexes using StringIndexer, then we use OneHotEnCoder to perform the enconding process.</p>\n<p>Below is an example of the full process to transform Major to one hot codes. We first import the two needed classes, then use fit() to train and transform() to perform the transformation</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1674595176367_802565321","id":"20230124-160750_2105055651","dateCreated":"2023-01-24T21:19:36+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7545"},{"text":"%spark2.pyspark\n\nfrom pyspark.ml.feature import OneHotEncoder\nfrom pyspark.ml.feature import StringIndexer\n\nMajorIndexer = StringIndexer(inputCol=\"Major\", outputCol=\"MajorIndex\")\n\nindexed = MajorIndexer.fit(students_train).transform(students_train)\n\noheMajor = OneHotEncoder(inputCol='MajorIndex', outputCol='MajorCodes')\n\nencoded = oheMajor.transform(indexed)","user":"anonymous","dateUpdated":"2023-01-24T21:19:36+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1674595176368_-81389117","id":"20230124-160633_1707339838","dateCreated":"2023-01-24T21:19:36+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7546"},{"text":"%spark2.pyspark\n\nindexed.printSchema()","user":"anonymous","dateUpdated":"2023-01-24T21:19:36+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- Major: string (nullable = true)\n |-- HighSchoolGPA: double (nullable = true)\n |-- FamilyIncome: double (nullable = true)\n |-- State: string (nullable = true)\n |-- AvgDailyStudyTime: double (nullable = true)\n |-- TotalAbsence: double (nullable = true)\n |-- isHonor: double (nullable = true)\n |-- MajorIndex: double (nullable = false)\n\n"}]},"apps":[],"jobName":"paragraph_1674595176368_-42836941","id":"20230124-161723_1600043207","dateCreated":"2023-01-24T21:19:36+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7547"},{"text":"%spark2.pyspark\n\nencoded.printSchema()","user":"anonymous","dateUpdated":"2023-01-24T21:19:36+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- Major: string (nullable = true)\n |-- HighSchoolGPA: double (nullable = true)\n |-- FamilyIncome: double (nullable = true)\n |-- State: string (nullable = true)\n |-- AvgDailyStudyTime: double (nullable = true)\n |-- TotalAbsence: double (nullable = true)\n |-- isHonor: double (nullable = true)\n |-- MajorIndex: double (nullable = false)\n |-- MajorCodes: vector (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1674595176368_-322857696","id":"20230124-162814_1915676134","dateCreated":"2023-01-24T21:19:36+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7548"},{"text":"%spark2.pyspark\nencoded.show()","user":"anonymous","dateUpdated":"2023-01-24T21:19:36+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----------------+-------------+------------+-----+-----------------+------------+-------+----------+-------------+\n|           Major|HighSchoolGPA|FamilyIncome|State|AvgDailyStudyTime|TotalAbsence|isHonor|MajorIndex|   MajorCodes|\n+----------------+-------------+------------+-----+-----------------+------------+-------+----------+-------------+\n|Computer Science|         1.76|     22345.0|   GA|             2.86|        32.0|    0.0|       1.0|(3,[1],[1.0])|\n|Computer Science|         1.84|     95221.0|   SC|             0.85|        21.0|    0.0|       1.0|(3,[1],[1.0])|\n|Computer Science|         2.07|    123967.0|   GA|             1.27|        29.0|    0.0|       1.0|(3,[1],[1.0])|\n|Computer Science|         2.09|    149181.0|   GA|             1.15|        27.0|    0.0|       1.0|(3,[1],[1.0])|\n|Computer Science|         2.11|     41759.0|   FL|             4.57|        31.0|    0.0|       1.0|(3,[1],[1.0])|\n|Computer Science|         2.12|     37414.0|   GA|             null|        16.0|    0.0|       1.0|(3,[1],[1.0])|\n|Computer Science|         2.13|    448877.0|   AL|             4.64|        27.0|    0.0|       1.0|(3,[1],[1.0])|\n|Computer Science|         2.19|    281197.0|   GA|             7.69|        26.0|    0.0|       1.0|(3,[1],[1.0])|\n|Computer Science|         2.21|     35728.0|   GA|             2.38|        29.0|    0.0|       1.0|(3,[1],[1.0])|\n|Computer Science|         2.21|     38482.0|   GA|             1.92|        16.0|    0.0|       1.0|(3,[1],[1.0])|\n|Computer Science|         2.23|     57436.0|   SC|             6.59|        11.0|    0.0|       1.0|(3,[1],[1.0])|\n|Computer Science|         2.31|     60898.0|   FL|             3.31|        21.0|    0.0|       1.0|(3,[1],[1.0])|\n|Computer Science|         2.32|     27297.0|   SC|             1.94|        23.0|    0.0|       1.0|(3,[1],[1.0])|\n|Computer Science|         2.32|     72319.0|   GA|             6.69|        19.0|    0.0|       1.0|(3,[1],[1.0])|\n|Computer Science|         2.35|     48140.0|   GA|             3.37|        20.0|    0.0|       1.0|(3,[1],[1.0])|\n|Computer Science|         2.35|    185087.0|   SC|             2.52|        23.0|    0.0|       1.0|(3,[1],[1.0])|\n|Computer Science|         2.36|    395839.0|   GA|             3.77|        26.0|    0.0|       1.0|(3,[1],[1.0])|\n|Computer Science|         2.37|    374066.0|   GA|             8.28|        23.0|    0.0|       1.0|(3,[1],[1.0])|\n|Computer Science|         2.39|     90581.0|   TN|             1.64|        23.0|    0.0|       1.0|(3,[1],[1.0])|\n|Computer Science|         2.39|    114049.0|   FL|             0.42|        28.0|    0.0|       1.0|(3,[1],[1.0])|\n+----------------+-------------+------------+-----+-----------------+------------+-------+----------+-------------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1674595176368_2049442032","id":"20230124-170350_1425513955","dateCreated":"2023-01-24T21:19:36+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7549"},{"text":"%md\n\n<h3>Missing Data</h3>\n\nMissing data is a major issue if they present. Most models cannot handle missing values and will just drop rows with them which will bias the analysis.\n\nA common strategy is to replace the missing data with their columns' median. In PySpark, we use the Imputer class for this purpose. Also note that Imputer is used for numeric columns only.\n\nIn the code below, first we import and create an Imputer object with input set to all numeric columns in the students data. strategy='median' indicates to use columns' medians to fill missing/null values\n- Also note the list of output columns which changes the names of imputed columns. We will need to use the new names in later steps.\n","user":"anonymous","dateUpdated":"2023-01-24T21:19:36+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Missing Data</h3>\n<p>Missing data is a major issue if they present. Most models cannot handle missing values and will just drop rows with them which will bias the analysis.</p>\n<p>A common strategy is to replace the missing data with their columns&rsquo; median. In PySpark, we use the Imputer class for this purpose. Also note that Imputer is used for numeric columns only.</p>\n<p>In the code below, first we import and create an Imputer object with input set to all numeric columns in the students data. strategy=&lsquo;median&rsquo; indicates to use columns&rsquo; medians to fill missing/null values<br/>- Also note the list of output columns which changes the names of imputed columns. We will need to use the new names in later steps.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1674595176368_1918418916","id":"20230124-204517_1171934591","dateCreated":"2023-01-24T21:19:36+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7550"},{"text":"%spark2.pyspark\n\nfrom pyspark.ml.feature import Imputer\n\nimputer = Imputer(inputCols = ['HighSchoolGPA','FamilyIncome','AvgDailyStudyTime','TotalAbsence'], outputCols = ['HighSchoolGPAImp','FamilyIncomeImp','AvgDailyStudyTimeImp','TotalAbsenceImp'], strategy = 'median')\nimputed = imputer.fit(encoded).transform(encoded)","user":"anonymous","dateUpdated":"2023-01-24T21:19:36+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1674595176368_-1509781922","id":"20230124-165117_1046257613","dateCreated":"2023-01-24T21:19:36+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7551"},{"text":"%spark2.pyspark\n\nimputed.show()","user":"anonymous","dateUpdated":"2023-01-24T21:19:36+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----------------+-------------+------------+-----+-----------------+------------+-------+----------+-------------+----------------+---------------+--------------------+---------------+\n|           Major|HighSchoolGPA|FamilyIncome|State|AvgDailyStudyTime|TotalAbsence|isHonor|MajorIndex|   MajorCodes|HighSchoolGPAImp|FamilyIncomeImp|AvgDailyStudyTimeImp|TotalAbsenceImp|\n+----------------+-------------+------------+-----+-----------------+------------+-------+----------+-------------+----------------+---------------+--------------------+---------------+\n|Computer Science|         1.76|     22345.0|   GA|             2.86|        32.0|    0.0|       1.0|(3,[1],[1.0])|            1.76|        22345.0|                2.86|           32.0|\n|Computer Science|         1.84|     95221.0|   SC|             0.85|        21.0|    0.0|       1.0|(3,[1],[1.0])|            1.84|        95221.0|                0.85|           21.0|\n|Computer Science|         2.07|    123967.0|   GA|             1.27|        29.0|    0.0|       1.0|(3,[1],[1.0])|            2.07|       123967.0|                1.27|           29.0|\n|Computer Science|         2.09|    149181.0|   GA|             1.15|        27.0|    0.0|       1.0|(3,[1],[1.0])|            2.09|       149181.0|                1.15|           27.0|\n|Computer Science|         2.11|     41759.0|   FL|             4.57|        31.0|    0.0|       1.0|(3,[1],[1.0])|            2.11|        41759.0|                4.57|           31.0|\n|Computer Science|         2.12|     37414.0|   GA|             null|        16.0|    0.0|       1.0|(3,[1],[1.0])|            2.12|        37414.0|                6.24|           16.0|\n|Computer Science|         2.13|    448877.0|   AL|             4.64|        27.0|    0.0|       1.0|(3,[1],[1.0])|            2.13|       448877.0|                4.64|           27.0|\n|Computer Science|         2.19|    281197.0|   GA|             7.69|        26.0|    0.0|       1.0|(3,[1],[1.0])|            2.19|       281197.0|                7.69|           26.0|\n|Computer Science|         2.21|     35728.0|   GA|             2.38|        29.0|    0.0|       1.0|(3,[1],[1.0])|            2.21|        35728.0|                2.38|           29.0|\n|Computer Science|         2.21|     38482.0|   GA|             1.92|        16.0|    0.0|       1.0|(3,[1],[1.0])|            2.21|        38482.0|                1.92|           16.0|\n|Computer Science|         2.23|     57436.0|   SC|             6.59|        11.0|    0.0|       1.0|(3,[1],[1.0])|            2.23|        57436.0|                6.59|           11.0|\n|Computer Science|         2.31|     60898.0|   FL|             3.31|        21.0|    0.0|       1.0|(3,[1],[1.0])|            2.31|        60898.0|                3.31|           21.0|\n|Computer Science|         2.32|     27297.0|   SC|             1.94|        23.0|    0.0|       1.0|(3,[1],[1.0])|            2.32|        27297.0|                1.94|           23.0|\n|Computer Science|         2.32|     72319.0|   GA|             6.69|        19.0|    0.0|       1.0|(3,[1],[1.0])|            2.32|        72319.0|                6.69|           19.0|\n|Computer Science|         2.35|     48140.0|   GA|             3.37|        20.0|    0.0|       1.0|(3,[1],[1.0])|            2.35|        48140.0|                3.37|           20.0|\n|Computer Science|         2.35|    185087.0|   SC|             2.52|        23.0|    0.0|       1.0|(3,[1],[1.0])|            2.35|       185087.0|                2.52|           23.0|\n|Computer Science|         2.36|    395839.0|   GA|             3.77|        26.0|    0.0|       1.0|(3,[1],[1.0])|            2.36|       395839.0|                3.77|           26.0|\n|Computer Science|         2.37|    374066.0|   GA|             8.28|        23.0|    0.0|       1.0|(3,[1],[1.0])|            2.37|       374066.0|                8.28|           23.0|\n|Computer Science|         2.39|     90581.0|   TN|             1.64|        23.0|    0.0|       1.0|(3,[1],[1.0])|            2.39|        90581.0|                1.64|           23.0|\n|Computer Science|         2.39|    114049.0|   FL|             0.42|        28.0|    0.0|       1.0|(3,[1],[1.0])|            2.39|       114049.0|                0.42|           28.0|\n+----------------+-------------+------------+-----+-----------------+------------+-------+----------+-------------+----------------+---------------+--------------------+---------------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1674595176368_115964276","id":"20230124-165420_664250757","dateCreated":"2023-01-24T21:19:36+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7552"},{"text":"%md\n\n<h3>Assembling All Features</h3>\n\nParticularly in Spark, all input features for a model must be placed in a single vector for each row. This is done with the VectorAssembler class which is showed in the example below\n- Note that now we only include columns we want to keep for modeling, which are encoded string columns, and imputed numeric columns\n","user":"anonymous","dateUpdated":"2023-01-24T21:19:36+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Assembling All Features</h3>\n<p>Particularly in Spark, all input features for a model must be placed in a single vector for each row. This is done with the VectorAssembler class which is showed in the example below<br/>- Note that now we only include columns we want to keep for modeling, which are encoded string columns, and imputed numeric columns</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1674595176368_-1890533418","id":"20230124-204722_344519343","dateCreated":"2023-01-24T21:19:36+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7553"},{"text":"%spark2.pyspark\n\nfrom pyspark.ml.feature import VectorAssembler\n\nva = VectorAssembler(inputCols=['MajorCodes','HighSchoolGPAImp','FamilyIncomeImp','AvgDailyStudyTimeImp','TotalAbsenceImp'], outputCol='features')\nfinal_data = va.transform(imputed)\nfinal_data.show()","user":"anonymous","dateUpdated":"2023-01-24T21:19:36+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----------------+-------------+------------+-----+-----------------+------------+-------+----------+-------------+----------------+---------------+--------------------+---------------+--------------------+\n|           Major|HighSchoolGPA|FamilyIncome|State|AvgDailyStudyTime|TotalAbsence|isHonor|MajorIndex|   MajorCodes|HighSchoolGPAImp|FamilyIncomeImp|AvgDailyStudyTimeImp|TotalAbsenceImp|            features|\n+----------------+-------------+------------+-----+-----------------+------------+-------+----------+-------------+----------------+---------------+--------------------+---------------+--------------------+\n|Computer Science|         1.76|     22345.0|   GA|             2.86|        32.0|    0.0|       1.0|(3,[1],[1.0])|            1.76|        22345.0|                2.86|           32.0|[0.0,1.0,0.0,1.76...|\n|Computer Science|         1.84|     95221.0|   SC|             0.85|        21.0|    0.0|       1.0|(3,[1],[1.0])|            1.84|        95221.0|                0.85|           21.0|[0.0,1.0,0.0,1.84...|\n|Computer Science|         2.07|    123967.0|   GA|             1.27|        29.0|    0.0|       1.0|(3,[1],[1.0])|            2.07|       123967.0|                1.27|           29.0|[0.0,1.0,0.0,2.07...|\n|Computer Science|         2.09|    149181.0|   GA|             1.15|        27.0|    0.0|       1.0|(3,[1],[1.0])|            2.09|       149181.0|                1.15|           27.0|[0.0,1.0,0.0,2.09...|\n|Computer Science|         2.11|     41759.0|   FL|             4.57|        31.0|    0.0|       1.0|(3,[1],[1.0])|            2.11|        41759.0|                4.57|           31.0|[0.0,1.0,0.0,2.11...|\n|Computer Science|         2.12|     37414.0|   GA|             null|        16.0|    0.0|       1.0|(3,[1],[1.0])|            2.12|        37414.0|                6.24|           16.0|[0.0,1.0,0.0,2.12...|\n|Computer Science|         2.13|    448877.0|   AL|             4.64|        27.0|    0.0|       1.0|(3,[1],[1.0])|            2.13|       448877.0|                4.64|           27.0|[0.0,1.0,0.0,2.13...|\n|Computer Science|         2.19|    281197.0|   GA|             7.69|        26.0|    0.0|       1.0|(3,[1],[1.0])|            2.19|       281197.0|                7.69|           26.0|[0.0,1.0,0.0,2.19...|\n|Computer Science|         2.21|     35728.0|   GA|             2.38|        29.0|    0.0|       1.0|(3,[1],[1.0])|            2.21|        35728.0|                2.38|           29.0|[0.0,1.0,0.0,2.21...|\n|Computer Science|         2.21|     38482.0|   GA|             1.92|        16.0|    0.0|       1.0|(3,[1],[1.0])|            2.21|        38482.0|                1.92|           16.0|[0.0,1.0,0.0,2.21...|\n|Computer Science|         2.23|     57436.0|   SC|             6.59|        11.0|    0.0|       1.0|(3,[1],[1.0])|            2.23|        57436.0|                6.59|           11.0|[0.0,1.0,0.0,2.23...|\n|Computer Science|         2.31|     60898.0|   FL|             3.31|        21.0|    0.0|       1.0|(3,[1],[1.0])|            2.31|        60898.0|                3.31|           21.0|[0.0,1.0,0.0,2.31...|\n|Computer Science|         2.32|     27297.0|   SC|             1.94|        23.0|    0.0|       1.0|(3,[1],[1.0])|            2.32|        27297.0|                1.94|           23.0|[0.0,1.0,0.0,2.32...|\n|Computer Science|         2.32|     72319.0|   GA|             6.69|        19.0|    0.0|       1.0|(3,[1],[1.0])|            2.32|        72319.0|                6.69|           19.0|[0.0,1.0,0.0,2.32...|\n|Computer Science|         2.35|     48140.0|   GA|             3.37|        20.0|    0.0|       1.0|(3,[1],[1.0])|            2.35|        48140.0|                3.37|           20.0|[0.0,1.0,0.0,2.35...|\n|Computer Science|         2.35|    185087.0|   SC|             2.52|        23.0|    0.0|       1.0|(3,[1],[1.0])|            2.35|       185087.0|                2.52|           23.0|[0.0,1.0,0.0,2.35...|\n|Computer Science|         2.36|    395839.0|   GA|             3.77|        26.0|    0.0|       1.0|(3,[1],[1.0])|            2.36|       395839.0|                3.77|           26.0|[0.0,1.0,0.0,2.36...|\n|Computer Science|         2.37|    374066.0|   GA|             8.28|        23.0|    0.0|       1.0|(3,[1],[1.0])|            2.37|       374066.0|                8.28|           23.0|[0.0,1.0,0.0,2.37...|\n|Computer Science|         2.39|     90581.0|   TN|             1.64|        23.0|    0.0|       1.0|(3,[1],[1.0])|            2.39|        90581.0|                1.64|           23.0|[0.0,1.0,0.0,2.39...|\n|Computer Science|         2.39|    114049.0|   FL|             0.42|        28.0|    0.0|       1.0|(3,[1],[1.0])|            2.39|       114049.0|                0.42|           28.0|[0.0,1.0,0.0,2.39...|\n+----------------+-------------+------------+-----+-----------------+------------+-------+----------+-------------+----------------+---------------+--------------------+---------------+--------------------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1674595176368_1866103255","id":"20230124-175543_509549821","dateCreated":"2023-01-24T21:19:36+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7554"},{"text":"%md\n\n<h3>Processing Pipeline</h3>\n\nProcessing data step-by-step is fine however not too convenient, especially if we have to repeat the same process multiple times.\n\nInstead, we will utilize the **Pipeline** class. A pipeline allows mutiple processing steps to be wrapped inside a single object that is reusable.\n\nThe below code for pipeline is pretty standard. You can **reuse the code in different analysis**, just make sure to change the list of numeric columns to impute, as well as add more StringIndexer and OneHotEncoder for each string column.\n- As can be seen, I added a pair of Indexer/Encoder for column State. I also added handleInvalid=\"keep\" to the Indexers to avoid errors with missing data in general.\n\nIn general, the objects listed in stages=[] for Pipeline are not strictly order. You just need to make sure for each string class, the Indexer comes before the Encoder, and the assembler is always at the end.\n\nAfter creating the pipeline, we train it with fit() and store the trained pipeline in a new variable.","user":"anonymous","dateUpdated":"2023-01-24T21:19:36+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Processing Pipeline</h3>\n<p>Processing data step-by-step is fine however not too convenient, especially if we have to repeat the same process multiple times.</p>\n<p>Instead, we will utilize the <strong>Pipeline</strong> class. A pipeline allows mutiple processing steps to be wrapped inside a single object that is reusable.</p>\n<p>The below code for pipeline is pretty standard. You can <strong>reuse the code in different analysis</strong>, just make sure to change the list of numeric columns to impute, as well as add more StringIndexer and OneHotEncoder for each string column.<br/>- As can be seen, I added a pair of Indexer/Encoder for column State. I also added handleInvalid=&ldquo;keep&rdquo; to the Indexers to avoid errors with missing data in general.</p>\n<p>In general, the objects listed in stages=[] for Pipeline are not strictly order. You just need to make sure for each string class, the Indexer comes before the Encoder, and the assembler is always at the end.</p>\n<p>After creating the pipeline, we train it with fit() and store the trained pipeline in a new variable.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1674595176368_-1469641600","id":"20230124-205038_993821482","dateCreated":"2023-01-24T21:19:36+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7555"},{"text":"%spark2.pyspark\nfrom pyspark.ml import Pipeline\n\nMajorIndexer = StringIndexer(inputCol=\"Major\", outputCol=\"MajorIndex\", handleInvalid=\"keep\")\noheMajor = OneHotEncoder(inputCol='MajorIndex', outputCol='MajorCodes')\n\nStateIndexer = StringIndexer(inputCol=\"State\", outputCol=\"StateIndex\", handleInvalid=\"keep\")\noheState = OneHotEncoder(inputCol='StateIndex', outputCol='StateCodes')\n\nimputer = Imputer(inputCols = ['HighSchoolGPA','FamilyIncome','AvgDailyStudyTime','TotalAbsence'], outputCols = ['HighSchoolGPAImp','FamilyIncomeImp','AvgDailyStudyTimeImp','TotalAbsenceImp'], strategy = 'median')\n\nassembler = VectorAssembler(inputCols=['MajorCodes','StateCodes','HighSchoolGPAImp','FamilyIncomeImp','AvgDailyStudyTimeImp','TotalAbsenceImp'], outputCol='features')\n\npipeline = Pipeline(stages = [MajorIndexer, oheMajor, StateIndexer, oheState, imputer, assembler])\n\npipeline_trained = pipeline.fit(students_train)","user":"anonymous","dateUpdated":"2023-01-24T21:19:36+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1674595176369_367301918","id":"20230124-171508_406577413","dateCreated":"2023-01-24T21:19:36+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7556"},{"text":"%md\n\nThe trained pipeline can now be used to transform data, whether in the training set or testing set. We further use select('isHonor','features') to remove all unneccessary columns (i.e., unprocessed or intermediate-processed columns)\n","user":"anonymous","dateUpdated":"2023-01-24T21:19:36+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>The trained pipeline can now be used to transform data, whether in the training set or testing set. We further use select(&lsquo;isHonor&rsquo;,&lsquo;features&rsquo;) to remove all unneccessary columns (i.e., unprocessed or intermediate-processed columns)</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1674595176369_-2109418769","id":"20230124-210021_1559696435","dateCreated":"2023-01-24T21:19:36+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7557"},{"text":"%spark2.pyspark\n\ntrain_prc = pipeline_trained.transform(students_train).select('isHonor','features')\n\ntrain_prc.show()","user":"anonymous","dateUpdated":"2023-01-24T21:19:36+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------+--------------------+\n|isHonor|            features|\n+-------+--------------------+\n|    0.0|(13,[1,4,9,10,11,...|\n|    0.0|(13,[1,5,9,10,11,...|\n|    0.0|(13,[1,4,9,10,11,...|\n|    0.0|(13,[1,4,9,10,11,...|\n|    0.0|(13,[1,6,9,10,11,...|\n|    0.0|(13,[1,4,9,10,11,...|\n|    0.0|(13,[1,7,9,10,11,...|\n|    0.0|(13,[1,4,9,10,11,...|\n|    0.0|(13,[1,4,9,10,11,...|\n|    0.0|(13,[1,4,9,10,11,...|\n|    0.0|(13,[1,5,9,10,11,...|\n|    0.0|(13,[1,6,9,10,11,...|\n|    0.0|(13,[1,5,9,10,11,...|\n|    0.0|(13,[1,4,9,10,11,...|\n|    0.0|(13,[1,4,9,10,11,...|\n|    0.0|(13,[1,5,9,10,11,...|\n|    0.0|(13,[1,4,9,10,11,...|\n|    0.0|(13,[1,4,9,10,11,...|\n|    0.0|(13,[1,8,9,10,11,...|\n|    0.0|(13,[1,6,9,10,11,...|\n+-------+--------------------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1674595176369_-471471499","id":"20230124-175256_1002768516","dateCreated":"2023-01-24T21:19:36+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7558"},{"text":"%spark2.pyspark\ntest_prc = pipeline_trained.transform(students_test).select('isHonor','features')\n\ntest_prc.show()","user":"anonymous","dateUpdated":"2023-01-24T21:19:36+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------+--------------------+\n|isHonor|            features|\n+-------+--------------------+\n|    0.0|(13,[1,4,9,10,12]...|\n|    0.0|(13,[1,7,9,10,11,...|\n|    0.0|(13,[1,7,9,10,11,...|\n|    0.0|(13,[1,6,9,10,11,...|\n|    0.0|(13,[1,7,9,10,11,...|\n|    0.0|(13,[1,5,9,10,11,...|\n|    0.0|(13,[1,4,9,10,11,...|\n|    0.0|(13,[1,7,9,10,11,...|\n|    0.0|(13,[1,4,9,10,11,...|\n|    0.0|(13,[1,4,9,10,11,...|\n|    0.0|(13,[1,5,9,10,11,...|\n|    0.0|(13,[1,4,9,10,11,...|\n|    0.0|(13,[1,9,10,11,12...|\n|    0.0|(13,[1,4,9,10,11,...|\n|    0.0|(13,[1,4,9,10,11,...|\n|    0.0|(13,[1,5,9,10,11,...|\n|    0.0|(13,[1,4,9,10,11,...|\n|    0.0|(13,[1,4,9,10,11,...|\n|    0.0|(13,[1,8,9,10,11,...|\n|    0.0|(13,[1,8,9,10,11,...|\n+-------+--------------------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1674595176369_1400588265","id":"20230124-180620_157977192","dateCreated":"2023-01-24T21:19:36+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7559"},{"text":"%md\n\n<h2>Modeling</h2>\n\nAfter data is fully processed, it is time to build models. While this is the contents for next week, the code below demonstrate how to build, train, and use a simple model to make prediction.\n\nWe first create a Logistic Regression model and define its input (featuresCol) and output (labelCol). The fit() function then trains the model.","user":"anonymous","dateUpdated":"2023-01-24T21:37:53+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Modeling</h2>\n<p>After data is fully processed, it is time to build models. While this is the contents for next week, the code below demonstrate how to build, train, and use a simple model to make prediction.</p>\n<p>We first create a Logistic Regression model and define its input (featuresCol) and output (labelCol). The fit() function then trains the model.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1674595176369_1575368948","id":"20230124-211226_580908956","dateCreated":"2023-01-24T21:19:36+0000","dateStarted":"2023-01-24T21:37:53+0000","dateFinished":"2023-01-24T21:37:54+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7560"},{"text":"%spark2.pyspark\n\nfrom pyspark.ml.classification import LogisticRegression\n\nlogistic_model = LogisticRegression(featuresCol='features', labelCol='isHonor')\n\nlogistic_trained = logistic_model.fit(train_prc)","user":"anonymous","dateUpdated":"2023-01-24T21:19:36+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1674595176369_-175353238","id":"20230124-181125_1302286594","dateCreated":"2023-01-24T21:19:36+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7561"},{"text":"%md\n\nThe trained model can be used to make prediction with transform(). The result is a new DataFrame with a column prediction added. Prediction will have the predicted values for the rows","user":"anonymous","dateUpdated":"2023-01-24T21:38:54+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>The trained model can be used to make prediction with transform(). The result is a new DataFrame with a column prediction added. Prediction will have the predicted values for the rows</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1674596279587_795452761","id":"20230124-213759_108963344","dateCreated":"2023-01-24T21:37:59+0000","dateStarted":"2023-01-24T21:38:54+0000","dateFinished":"2023-01-24T21:38:54+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7562"},{"text":"%spark2.pyspark\n\ntrain_predicted = logistic_trained.transform(train_prc)\ntrain_predicted.show()","user":"anonymous","dateUpdated":"2023-01-24T21:19:36+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------+--------------------+--------------------+--------------------+----------+\n|isHonor|            features|       rawPrediction|         probability|prediction|\n+-------+--------------------+--------------------+--------------------+----------+\n|    0.0|(13,[1,4,9,10,11,...|[10.1774236921902...|[0.99996198241379...|       0.0|\n|    0.0|(13,[1,5,9,10,11,...|[12.9238526172910...|[0.99999756083564...|       0.0|\n|    0.0|(13,[1,4,9,10,11,...|[11.9835995214259...|[0.99999375422777...|       0.0|\n|    0.0|(13,[1,4,9,10,11,...|[11.9876600435198...|[0.99999377953729...|       0.0|\n|    0.0|(13,[1,6,9,10,11,...|[7.5260654267305,...|[0.99946143595480...|       0.0|\n|    0.0|(13,[1,4,9,10,11,...|[4.31821108653142...|[0.98685148959382...|       0.0|\n|    0.0|(13,[1,7,9,10,11,...|[6.40575259253236...|[0.99835069823917...|       0.0|\n|    0.0|(13,[1,4,9,10,11,...|[2.60790490136863...|[0.93136859616690...|       0.0|\n|    0.0|(13,[1,4,9,10,11,...|[10.4976215925632...|[0.99997239874165...|       0.0|\n|    0.0|(13,[1,4,9,10,11,...|[10.3150897785736...|[0.99996687170832...|       0.0|\n|    0.0|(13,[1,5,9,10,11,...|[4.20575146063236...|[0.98530945197266...|       0.0|\n|    0.0|(13,[1,6,9,10,11,...|[8.56526659806736...|[0.99980942356164...|       0.0|\n|    0.0|(13,[1,5,9,10,11,...|[11.4587660903891...|[0.99998944358310...|       0.0|\n|    0.0|(13,[1,4,9,10,11,...|[3.77351644380752...|[0.97754468003763...|       0.0|\n|    0.0|(13,[1,4,9,10,11,...|[8.48829525947073...|[0.99979417846553...|       0.0|\n|    0.0|(13,[1,5,9,10,11,...|[10.4480941268563...|[0.99997099734353...|       0.0|\n|    0.0|(13,[1,4,9,10,11,...|[7.88304257057995...|[0.99962305813381...|       0.0|\n|    0.0|(13,[1,4,9,10,11,...|[1.42488572047320...|[0.80610320081603...|       0.0|\n|    0.0|(13,[1,8,9,10,11,...|[10.8767493139859...|[0.99998110792821...|       0.0|\n|    0.0|(13,[1,6,9,10,11,...|[12.9490109683589...|[0.99999762143536...|       0.0|\n+-------+--------------------+--------------------+--------------------+----------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1674595176369_1006693846","id":"20230124-182424_782997468","dateCreated":"2023-01-24T21:19:36+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7563"},{"text":"%md\n\nFinall, we can use crosstab() from the predicted dataframe to obtained a <b>confusion matrix</b>. This is a table that shows the number of rows with their label predicted correctly or incorrectly.","user":"anonymous","dateUpdated":"2023-01-24T21:41:25+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Finall, we can use crosstab() from the predicted dataframe to obtained a <b>confusion matrix</b>. This is a table that shows the number of rows with their label predicted correctly or incorrectly.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1674596339648_202262955","id":"20230124-213859_1869258115","dateCreated":"2023-01-24T21:38:59+0000","dateStarted":"2023-01-24T21:41:26+0000","dateFinished":"2023-01-24T21:41:26+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7564"},{"text":"%spark2.pyspark\ntrain_predicted.crosstab('isHonor', 'prediction').show()","user":"anonymous","dateUpdated":"2023-01-24T21:19:36+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+------------------+---+---+\n|isHonor_prediction|0.0|1.0|\n+------------------+---+---+\n|               1.0| 46|106|\n|               0.0|502| 23|\n+------------------+---+---+\n\n"}]},"apps":[],"jobName":"paragraph_1674595176369_-2087458303","id":"20230124-184539_1694421190","dateCreated":"2023-01-24T21:19:36+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7565"},{"text":"%md\n\nPredictions can be made for the testing data exactly the same.\n","user":"anonymous","dateUpdated":"2023-01-24T21:41:46+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Predictions can be made for the testing data exactly the same.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1674596490208_-2047716195","id":"20230124-214130_355381721","dateCreated":"2023-01-24T21:41:30+0000","dateStarted":"2023-01-24T21:41:46+0000","dateFinished":"2023-01-24T21:41:46+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7566"},{"text":"%spark2.pyspark\ntest_predicted = logistic_trained.transform(test_prc)\ntest_predicted.show()","user":"anonymous","dateUpdated":"2023-01-24T21:19:36+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------+--------------------+--------------------+--------------------+----------+\n|isHonor|            features|       rawPrediction|         probability|prediction|\n+-------+--------------------+--------------------+--------------------+----------+\n|    0.0|(13,[1,4,9,10,12]...|[13.4720458371511...|[0.99999859017820...|       0.0|\n|    0.0|(13,[1,7,9,10,11,...|[10.5764634900064...|[0.99997449124927...|       0.0|\n|    0.0|(13,[1,7,9,10,11,...|[10.8215585301019...|[0.99998003597181...|       0.0|\n|    0.0|(13,[1,6,9,10,11,...|[6.75333242811815...|[0.99883437602205...|       0.0|\n|    0.0|(13,[1,7,9,10,11,...|[12.8303438897785...|[0.99999732174894...|       0.0|\n|    0.0|(13,[1,5,9,10,11,...|[8.33426031934439...|[0.9997599108961,...|       0.0|\n|    0.0|(13,[1,4,9,10,11,...|[5.37580976086511...|[0.99539414177580...|       0.0|\n|    0.0|(13,[1,7,9,10,11,...|[8.60213869687811...|[0.99981632127613...|       0.0|\n|    0.0|(13,[1,4,9,10,11,...|[1.18991054666397...|[0.76672506518880...|       0.0|\n|    0.0|(13,[1,4,9,10,11,...|[5.40706312345154...|[0.99553523138930...|       0.0|\n|    0.0|(13,[1,5,9,10,11,...|[6.22877313886412...|[0.99803201099430...|       0.0|\n|    0.0|(13,[1,4,9,10,11,...|[9.08838975503394...|[0.99988704292949...|       0.0|\n|    0.0|(13,[1,9,10,11,12...|[6.24838232952286...|[0.99807015201239...|       0.0|\n|    0.0|(13,[1,4,9,10,11,...|[9.17698455351148...|[0.99989661885266...|       0.0|\n|    0.0|(13,[1,4,9,10,11,...|[4.27589281724957...|[0.98629091787008...|       0.0|\n|    0.0|(13,[1,5,9,10,11,...|[4.12174134750370...|[0.98404251881947...|       0.0|\n|    0.0|(13,[1,4,9,10,11,...|[3.02960388502813...|[0.95389375477620...|       0.0|\n|    0.0|(13,[1,4,9,10,11,...|[12.5835783750602...|[0.99999657216538...|       0.0|\n|    0.0|(13,[1,8,9,10,11,...|[5.04603378933482...|[0.99360633727917...|       0.0|\n|    0.0|(13,[1,8,9,10,11,...|[8.04134617044033...|[0.99967822821116...|       0.0|\n+-------+--------------------+--------------------+--------------------+----------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1674595176369_-844532151","id":"20230124-183935_862994630","dateCreated":"2023-01-24T21:19:36+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7567"},{"text":"%spark2.pyspark\ntest_predicted.crosstab('isHonor', 'prediction').show()","user":"anonymous","dateUpdated":"2023-01-24T21:19:36+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+------------------+---+---+\n|isHonor_prediction|0.0|1.0|\n+------------------+---+---+\n|               1.0| 11| 39|\n|               0.0|261| 12|\n+------------------+---+---+\n\n"}]},"apps":[],"jobName":"paragraph_1674595176369_-1637183006","id":"20230124-184404_527645336","dateCreated":"2023-01-24T21:19:36+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7568"}],"name":"working with dataframe","id":"2HRC9A2GZ","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"spark2:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}